{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "455b2088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e739fda6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "16db9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines=[]\n",
    "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content']\n",
    "            entities = []\n",
    "            for annotation in data['annotation']:\n",
    "                #only a single point in text annotation.\n",
    "                point = annotation['points'][0]\n",
    "                labels = annotation['label']\n",
    "                # handle both list of labels or a single label.\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "\n",
    "                for label in labels:\n",
    "                    #dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n",
    "                    entities.append((point['start'], point['end'] + 1 ,label))\n",
    "            training_data.append((text, {\"entities\": entities}))\n",
    "        return training_data\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677268f3",
   "metadata": {},
   "source": [
    "# read manually annotated data into one json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "e28219e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_lst = ['../data/annotate_emmy/*','../data/annotation_zqw/*', '../data/annotations_LHB/*','../data/Archive_YSY/*']\n",
    "training_data = []\n",
    "def read_data(path):\n",
    "    first_is_special = r'^\\W[a-zA-Z0-9]+$'\n",
    "    last_is_special = r'^[a-zA-Z0-9]+\\W$'\n",
    "    json_lst = glob.glob(path)\n",
    "    for direc in json_lst:\n",
    "        f = open(direc)\n",
    "        data = json.load(f)\n",
    "        for text, entity in data['annotations']:\n",
    "            if len(entity['entities'])==0:\n",
    "                training_data.append((text, entity))\n",
    "            # add to training data only if there is a text\n",
    "            elif (len(text) != 0):\n",
    "                for x in entity['entities']:\n",
    "                    # do not append 'unlabelled' or 'unknown' entity\n",
    "                    if (x[2] == 'Unlabelled') or (x[2] == 'UNKNOWN'):\n",
    "                        continue\n",
    "                    else:\n",
    "                        training_data.append((text, entity))\n",
    "                        break\n",
    "        f.close()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "11b0aa6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/annotate_emmy/*\n",
      "../data/annotation_zqw/*\n",
      "../data/annotations_LHB/*\n",
      "../data/Archive_YSY/*\n"
     ]
    }
   ],
   "source": [
    "for x in path_lst:\n",
    "    print(x)\n",
    "    read_data(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "0fec880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_special_characters(data: list) -> list:\n",
    "\n",
    "    special_character = re.compile(r'\\W')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and special_character.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and special_character.match(\n",
    "                    text[valid_end - 1]) and text[valid_end-1]!='#':\n",
    "                valid_end -= 1\n",
    "\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "fa9e2e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = trim_special_characters(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "ce9d72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check any special character still exist\n",
    "entity_w_special_characters_first = []\n",
    "entity_w_special_characters_last = []\n",
    "first_is_special = r'^\\W[a-zA-Z0-9]+$'\n",
    "last_is_special = r'^[a-zA-Z0-9]+\\W$'\n",
    "for text, entity in training_data:\n",
    "    for x in entity['entities']:\n",
    "        if re.search(first_is_special, text[x[0]:x[1]]):\n",
    "            entity_w_special_characters_first.append(text[x[0]:x[1]])\n",
    "        if re.search(last_is_special, text[x[0]:x[1]]):\n",
    "            entity_w_special_characters_last.append(text[x[0]:x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "7264f105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C#', 'C#', 'C#', 'C#', 'C#', 'C#', 'C#', 'C#', 'C#']"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_w_special_characters_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "a94f0f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all types of entities\n",
    "entities = []\n",
    "for text, entity in training_data:\n",
    "    for start, end, entity_type in entity['entities']:\n",
    "        entities.append(entity_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "6a708eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11887"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "3f2f21e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['COLLEGE NAME', 'COMPANIES WORKED AT', 'DEGREE', 'DESIGNATION',\n",
       "        'DESIGNATION ', 'EMAIL ADDRESS', 'GRADUATION YEAR', 'LOCATION',\n",
       "        'NAME', 'SKILLS', 'UNKNOWN', 'Unlabelled', 'YEARS OF EXPERIENCE'],\n",
       "       dtype='<U19'),\n",
       " array([ 489,  864,  317,  785,  535,  171,  216, 1657,  218, 5746,    1,\n",
       "           1,  887]))"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(entities), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "9042ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export file\n",
    "with open('../data/manually_annotation.json', 'w') as f:\n",
    "    json.dump(training_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "24066206",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/manually_annotation.json', 'r') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "8353cdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7636\n"
     ]
    }
   ],
   "source": [
    "for line in lines:\n",
    "    data = json.loads(line)\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3e66cfa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Govardhana K\\nSenior Software Engineer\\n\\nBengaluru, Karnataka, Karnataka - Email me on Indeed: indeed.com/r/Govardhana-K/\\nb2de315d95905b68\\n\\nTotal IT experience 5 Years 6 Months\\nCloud Lending Solutions INC 4 Month • Salesforce Developer\\nOracle 5 Years 2 Month • Core Java Developer\\nLanguages Core Java, Go Lang\\nOracle PL-SQL programming,\\nSales Force Developer with APEX.\\n\\nDesignations & Promotions\\n\\nWilling to relocate: Anywhere\\n\\nWORK EXPERIENCE\\n\\nSenior Software Engineer\\n\\nCloud Lending Solutions -  Bangalore, Karnataka -\\n\\nJanuary 2018 to Present\\n\\nPresent\\n\\nSenior Consultant\\n\\nOracle -  Bangalore, Karnataka -\\n\\nNovember 2016 to December 2017\\n\\nStaff Consultant\\n\\nOracle -  Bangalore, Karnataka -\\n\\nJanuary 2014 to October 2016\\n\\nAssociate Consultant\\n\\nOracle -  Bangalore, Karnataka -\\n\\nNovember 2012 to December 2013\\n\\nEDUCATION\\n\\nB.E in Computer Science Engineering\\n\\nAdithya Institute of Technology -  Tamil Nadu\\n\\nSeptember 2008 to June 2012\\n\\nhttps://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN\\nhttps://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN\\n\\n\\nSKILLS\\n\\nAPEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years),\\nAlgorithms (3 years)\\n\\nLINKS\\n\\nhttps://www.linkedin.com/in/govardhana-k-61024944/\\n\\nADDITIONAL INFORMATION\\n\\nTechnical Proficiency:\\n\\nLanguages: Core Java, Go Lang, Data Structures & Algorithms, Oracle\\nPL-SQL programming, Sales Force with APEX.\\nTools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer,\\nPL/SQL Developer, WinSCP, Putty\\nWeb Technologies: JavaScript, XML, HTML, Webservice\\n\\nOperating Systems: Linux, Windows\\nVersion control system SVN & Git-Hub\\nDatabases: Oracle\\nMiddleware: Web logic, OC4J\\nProduct FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x\\n\\nhttps://www.linkedin.com/in/govardhana-k-61024944/',\n",
       " {'entities': [(1749, 1755, 'Companies worked at'),\n",
       "   (1696, 1702, 'Companies worked at'),\n",
       "   (1417, 1423, 'Companies worked at'),\n",
       "   (1356, 1793, 'Skills'),\n",
       "   (1209, 1215, 'Companies worked at'),\n",
       "   (1136, 1248, 'Skills'),\n",
       "   (928, 932, 'Graduation Year'),\n",
       "   (858, 889, 'College Name'),\n",
       "   (821, 856, 'Degree'),\n",
       "   (787, 791, 'Graduation Year'),\n",
       "   (744, 750, 'Companies worked at'),\n",
       "   (722, 742, 'Designation'),\n",
       "   (658, 664, 'Companies worked at'),\n",
       "   (640, 656, 'Designation'),\n",
       "   (574, 580, 'Companies worked at'),\n",
       "   (555, 573, 'Designation'),\n",
       "   (470, 493, 'Companies worked at'),\n",
       "   (444, 469, 'Designation'),\n",
       "   (308, 314, 'Companies worked at'),\n",
       "   (234, 240, 'Companies worked at'),\n",
       "   (175, 198, 'Companies worked at'),\n",
       "   (93, 137, 'Email Address'),\n",
       "   (39, 48, 'Location'),\n",
       "   (13, 38, 'Designation'),\n",
       "   (0, 12, 'Name')]})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the pre-collected data from github to match with the format\n",
    "train_fp = \"../test/traindata.json\"\n",
    "convert_dataturks_to_spacy(train_fp)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "65cfd0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_correction_dic = {'Email Address': 'EMAIL ADDRESS',\n",
    "                        'College Name': 'COLLEGE NAME',\n",
    "                        'Degree': 'DEGREE',\n",
    "                        'Location': 'LOCATION',\n",
    "                        'Skills': 'SKILLS',\n",
    "                        'Companies Worked at': 'COMPANIES WORKED AT',\n",
    "                        'Name': 'NAME',\n",
    "                        'Designation ': 'DESIGNATION',\n",
    "                        'Years of Experience': 'YEARS OF EXPERIENCE',\n",
    "                        'Graduation Year': 'GRADUATION YEAR'\n",
    "                        }\n",
    "'EMAIL ADDRESS' in label_correction_dic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
