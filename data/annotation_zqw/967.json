{"classes":["LOCATION","COLLEGE NAME","NAME","YEARS OF EXPERIENCE","DESIGNATION","DEGREE","EMAIL ADDRESS","SKILLS","GRADUATION YEAR","COMPANIES WORKED AT"],"annotations":[["ABDUL YUSUF\r",{"entities":[[0,11,"NAME"]]}],["adhgzy@r.postjobfree.com 571-***-****\r",{"entities":[[0,24,"EMAIL ADDRESS"]]}],["Data engineer with 8+ years of experience on Big Data/Hadoop and 9 years of experience in IT.\r",{"entities":[[0,13,"DESIGNATION"]]}],["Participate in the development/implementation of Cloudera Hadoop and Hortonworks environments.\r",{"entities":[]}],["Capture and transform real-time data into a suitable format for scalable analytics using DStreams.\r",{"entities":[]}],["Send requests to source REST-based API from Scala scripts via Kafka Producer.\r",{"entities":[]}],["Perform importing and exporting of data (SQL Server, CSV, TXT. Parquet, Avro) from local/external file system and RDBMS to S3.\r",{"entities":[[41,51,"SKILLS"]]}],["Write shell scripts for automating the process of data loading.\r",{"entities":[]}],["Utilize Spark for data processing and created DStreams from data received from Kafka.\r",{"entities":[[8,13,"SKILLS"],[46,54,"SKILLS"],[79,84,"SKILLS"]]}],["Store results of processed data in Hive.\r",{"entities":[[35,39,"SKILLS"]]}],["Automate AWS components like EC2 instances, Security groups, ELB, RDS, Lambda and IAM through AWS Cloud Formation templates.\r",{"entities":[[9,12,"SKILLS"]]}],["Work on large data warehouse analysis services servers and developed the different reports for the analysis from those servers.\r",{"entities":[]}],["Write Hive scripts to process HDFS data.\r",{"entities":[]}],["Experience developing Big Data systems with Hadoop and AWS EMR Hadoop clusters.\r",{"entities":[[55,58,"SKILLS"],[59,62,"SKILLS"],[63,69,"SKILLS"]]}],["Technically skilled in HDFS, Spark, Kafka, Hive, Sqoop, HBase, Flume, Oozie, Zookeeper, YARN, etc.\r",{"entities":[[23,27,"SKILLS"],[29,34,"SKILLS"],[36,41,"SKILLS"],[43,47,"SKILLS"],[49,54,"SKILLS"],[56,61,"SKILLS"],[63,68,"SKILLS"],[70,75,"SKILLS"],[77,86,"SKILLS"],[88,92,"SKILLS"]]}],["Skilled in Hadoop Big Data using Cloudera (CDH) and Hortonworks (HDP).\r",{"entities":[]}],["Mentor engineers with experience serving communications leadership functions maintaining ongoing communications with project stakeholders, business units, data scientists/analysts and making sure all teams collaborate smoothly.\r",{"entities":[]}],["Skilled extracting and generating data visualizations.\r",{"entities":[]}],["Facilitate meetings following Scrum processes such as Sprint Planning, Backlog, Sprint Retrospective, and Requirements Gathering.\r",{"entities":[]}],["Produce documentation for projects and ensure projects are on track with stakeholder wishes.\r",{"entities":[]}],["In-depth understanding/knowledge of Hadoop Architecture and various components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node and MapReduce concepts.\r",{"entities":[]}],["Utilize Apache Hadoop for working with Big Data to analyze large data sets efficiently.\r",{"entities":[]}],["Hands-on with Hive's analytical functions and extending Hive core functionality by writing custom SQL queries.\r",{"entities":[[98,101,"SKILLS"]]}],["Experience importing and exporting terabytes of data between HDFS and Relational Database Systems using Sqoop.\r",{"entities":[]}],["Hands-on applying Extract, Transform and Load (ETL) processes from databases such as SQL Server and Oracle to Hadoop HDFS in Data Lake.\r",{"entities":[]}],["Experience writing SQL queries, Stored Procedures, Triggers, Cursors and Packages.\r",{"entities":[]}],["Experience handling XML files and related technologies.\r",{"entities":[]}],["Skilled programming in Python, Scala, and Java.\r",{"entities":[[23,29,"SKILLS"],[31,36,"SKILLS"],[42,46,"SKILLS"]]}],["Apply Spark streaming to receive real-time data using Kafka/on prem and AWS cloud.\r",{"entities":[]}],["Use Spark Structured Streaming for high-performance, scalable, fault-tolerant data processing of real-time data streams by extending core Spark API on prem and AWS.\r",{"entities":[]}],["Experience with multiple terabytes of data stored in AWS using Elastic Map Reduce (EMR) and Redshift PostgreSQL.\r",{"entities":[]}],["Experience working Amazon Web Services (AWS) and Cloud Services such as EMR, EC2, S3, EBS and IAM entities, roles, and users.\r",{"entities":[]}],["SKILLS\r",{"entities":[]}],["APACHE\r",{"entities":[]}],["Ant, Flume, Hadoop, YARN, Hive, Kafka, MAVEN, Oozie, Spark, Tez, Zookeeper, Cloudera Impala, HDFS\r",{"entities":[]}],["Hortonworks, MapR, MapReduce\r",{"entities":[[0,11,"SKILLS"],[13,17,"SKILLS"],[19,28,"SKILLS"]]}],["SCRIPTING\r",{"entities":[]}],["HiveQL, MapReduce, XML, FTP,\r",{"entities":[[0,6,"SKILLS"],[8,17,"SKILLS"],[19,22,"SKILLS"],[24,27,"SKILLS"]]}],["Python, UNIX, Shell scripting, LINUX\r",{"entities":[[0,6,"SKILLS"],[8,12,"SKILLS"],[14,29,"SKILLS"],[31,36,"SKILLS"]]}],["OPERATING SYSTEMS\r",{"entities":[]}],["Unix/Linux, Windows 10, Ubuntu,\r",{"entities":[[0,10,"SKILLS"],[12,22,"SKILLS"],[24,30,"SKILLS"]]}],["FILE FORMATS\r",{"entities":[]}],["Parquet, Avro & JSON, ORC, text, CSV\r",{"entities":[]}],["DISTRIBUTIONS\r",{"entities":[]}],["Cloudera, Hortonworks, AWS, Elastic, ELK, Cloudera CDH, Hortonworks HDP, Amazon Web Services (AWS)\r",{"entities":[]}],["DATA PROCESSING (COMPUTE) ENGINES\r",{"entities":[]}],["Apache Spark, Spark Streaming\r",{"entities":[[0,12,"SKILLS"],[14,29,"SKILLS"]]}],["DATA VISUALIZATION TOOLS\r",{"entities":[]}],["Pentaho, QlikView, Tableau, PowerBI, Matplotlib, Plotly, Dash,\r",{"entities":[[0,7,"SKILLS"],[9,17,"SKILLS"],[19,26,"SKILLS"],[28,35,"SKILLS"],[37,47,"SKILLS"],[49,55,"SKILLS"],[57,61,"SKILLS"]]}],["COMPUTE ENGINES\r",{"entities":[]}],["Apache Spark, Spark Streaming, Storm\r",{"entities":[]}],["DATABASES\r",{"entities":[]}],["Microsoft SQL Server Database\r",{"entities":[[10,13,"SKILLS"]]}],["Database & Data Structures, Apache Cassandra, Amazon Redshift, DynamoDB, Apache HBase, Apache Hive, MongoDB,\r",{"entities":[]}],["SOFTWARE\r",{"entities":[]}],["Microsoft Project, Primavera P6, VMWare, Microsoft Word, Excel, Outlook, Power Point; Technical Documentation Skills\r",{"entities":[[0,17,"SKILLS"],[19,31,"SKILLS"],[33,39,"SKILLS"],[41,55,"SKILLS"],[57,62,"SKILLS"],[64,71,"SKILLS"],[73,84,"SKILLS"]]}],["WORK HISTORY\r",{"entities":[]}],["Hadoop Engineer Select Computing\r",{"entities":[[0,15,"DESIGNATION"]]}],["New York, NY November 2020 - Current\r",{"entities":[[0,8,"LOCATION"],[10,12,"LOCATION"]]}],["Contributed to the transformation of financial data stored on old mainframe computers to more recent methods of storing data with tie into an Oracle database.\r",{"entities":[]}],["Migrated data from mainframe computers in EBCDIC format to ASCII format/encoding,\r",{"entities":[]}],["Developed data pipeline that included files being stored on an AWS EC2 instance, decompressed and sent to AWS S3 bucket, and transformed to ASCII. Rules were applied to data based on DDL from client Oracle database admin. Date conversion/transformation applied by custom Python script amongst other data manipulation requirements.\r",{"entities":[]}],["Containerizing Confluent Kafka application and configured subnet for communication between containers.\r",{"entities":[]}],["Used Kafka to store data in topics available for consumers of the data.\r",{"entities":[]}],["Applied embedded JSON schemas to incoming data in Kafka and stored into another topic.\r",{"entities":[]}],["Interfaced with Oracle database using SQL Developer application to run queries of updated tables after transformation of data and verified correct DDL.\r",{"entities":[]}],["Used Lenses UI to visually check for tables before pushing to Oracle database.\r",{"entities":[]}],["Installed and configured Kafka, NIFI, Anaconda, and Oracle instant client onto several Linux servers.\r",{"entities":[]}],["Wrote scripts to ease manual repetitive tasks and automate procedures.\r",{"entities":[]}],["Installed Python packages used in Python transformation code.\r",{"entities":[]}],["Created repository in Bitbucket using Git commands.\r",{"entities":[]}],["Pushed updated Python code and other important files onto repository branch pulled from master branch.\r",{"entities":[]}],["Configured Anaconda virtual environment for running Python scripts.\r",{"entities":[]}],["Installed AWS command line interface (CLI) to interact with S3 bucket to download and upload files.\r",{"entities":[]}],["Attended daily meetings to review results of transformed tables and to see if they met requirements of client.\r",{"entities":[]}],["Ran Python scripts to initiate custom data pipeline used to download files, transform data within files, and upload to Oracle database server.\r",{"entities":[]}],["Mapped selected directory to make available to other servers within data pipeline using Network File System (NFS).\r",{"entities":[]}],["Generated SSH keys between servers for scripts to log in from one server to another to perform required tasks.\r",{"entities":[]}],["Configured Airflow and administered user and password credentials for several users.\r",{"entities":[]}],["Implemented and configured Docker Containers for Kafka data pipeline to work as intended.\r",{"entities":[]}],["Performed troubleshooting of memory issues on Linux server and applied corrective remedy by increasing Heap memory.\r",{"entities":[]}],["Data Engineer E-Trade\r",{"entities":[[0,13,"DESIGNATION"],[14,21,"COMPANIES WORKED AT"]]}],["Arlington, VA March 2019 â€“ November 2020\r",{"entities":[[0,9,"LOCATION"],[11,13,"LOCATION"]]}],["Split JSON files into DFs level to be processed in parallel for better performance and fault tolerance.\r",{"entities":[]}],["Created data frames in Apache Spark by passing schema as a parameter to the ingested data using case classes.\r",{"entities":[]}],["Participated in the development/implementation of Cloudera Hadoop and Hortonworks environments.\r",{"entities":[]}],["Captured and transformed real-time data into a suitable format for Scalable analytics using DStreams.\r",{"entities":[]}],["Sent requests to source REST Based API from a Scala script via Kafka Producer.\r",{"entities":[]}],["Deployed application jar files into AWS EMR instances.\r",{"entities":[]}],["Established a connection between the HBase and Spark for quick transfer of transactions.\r",{"entities":[]}],["populated data frames inside spark jobs, Spark SQL and Data Frames API to load structured data into Spark clusters.\r",{"entities":[]}],["Monitored background operations in Hortonworks Ambari.\r",{"entities":[]}],["Configured Zookeeper to coordinate the servers to maintain the data consistency and to monitor services.\r",{"entities":[]}],["Involved in analyzing system failures, identifying root causes and recommended course of actions.\r",{"entities":[]}],["Created standardized documents for company usage.\r",{"entities":[]}],["Created multi-node Hadoop and Spark clusters on EMR instances to process terabytes of data and stored it in AWS S3.\r",{"entities":[]}],["Involved in implementation of analytics solutions through Agile/Scrum processes for development and quality assurance.\r",{"entities":[]}],["Interacted with data residing in HDFS using Spark to process the data.\r",{"entities":[]}],["Worked one on one with clients to resolve issues regarding Spark jobs submissions.\r",{"entities":[]}],["Automated, configured, and deployed instances on AWS Azure environments.\r",{"entities":[[49,52,"SKILLS"],[53,58,"SKILLS"]]}],["Data Engineer iHeartRadio\r",{"entities":[[0,13,"DESIGNATION"],[14,25,"COMPANIES WORKED AT"]]}],["San Antonio, TX January 2018 - March 2019\r",{"entities":[[0,11,"LOCATION"],[13,15,"LOCATION"]]}],["Worked with iHeart API to retrieve data from host website.\r",{"entities":[]}],["Utilized AWS EMR Hadoop cluster to ingest data for processing.\r",{"entities":[[9,12,"SKILLS"],[13,16,"SKILLS"],[17,23,"SKILLS"]]}],["Developed Python code for spark processing and streaming of data.\r",{"entities":[[10,16,"SKILLS"],[26,31,"SKILLS"]]}],["Performed importing and exporting of data (SQL Server, csv, txt, parquet, avro) from local/external file system and RDBMS to S3.\r",{"entities":[]}],["Wrote shell scripts for automating the process of data loading.\r",{"entities":[]}],["Received JSON messages through Kafka consumer API in Python,\r",{"entities":[[31,36,"SKILLS"],[53,59,"SKILLS"]]}],["Formatted responses into data frames using a mutable schema to parse JSON.\r",{"entities":[]}],["Performed upgrades, patches and bug fixes in Hadoop in a cluster environment.\r",{"entities":[]}],["Integrated Kafka with Spark streaming for high-speed data processing.\r",{"entities":[[11,16,"SKILLS"],[22,27,"SKILLS"]]}],["Extracted real-time feed using Kafka and Spark Streaming and converted it to DStreams.\r",{"entities":[]}],["Managed and reviewed EMR log files.\r",{"entities":[]}],["Wrote Hive queries and optimized the Hive queries with Hive QL from external tables stored in S3.\r",{"entities":[]}],["Implemented and coded spark jobs in Python.\r",{"entities":[[36,42,"SKILLS"]]}],["Used Spark to load and process data stored on S3.\r",{"entities":[]}],["Stored data pulled from APIs into Apache Hive on EMR.\r",{"entities":[]}],["Hadoop Engineer Toyota\r",{"entities":[[0,15,"DESIGNATION"],[16,22,"COMPANIES WORKED AT"]]}],["Plano, TX October 2015 â€“ December 2017\r",{"entities":[[0,5,"LOCATION"],[7,9,"LOCATION"]]}],["Sourced data using APIs with data available in JSON to be converted to Parquet and Avro formats.\r",{"entities":[]}],["Used Kafka to ingest Data and create topics for data streaming.\r",{"entities":[]}],["Utilized Spark for data processing and creating DStreams from data received from Kafka.\r",{"entities":[]}],["Stored results of processed data in Hive.\r",{"entities":[]}],["Automated AWS components like EC2 instances, Security groups, ELB, RDS, Lambda and IAM through AWS Cloud Formation templates.\r",{"entities":[]}],["Worked on large data warehouse analysis services servers and developed the different reports for the analysis from those servers.\r",{"entities":[]}],["Wrote Hive scripts to process HDFS data.\r",{"entities":[]}],["Wrote shell scripts to automate workflows to pull data from various databases into Hadoop framework for users to access the data through Hive views.\r",{"entities":[]}],["Collected log data from various sources and integrated it into HDFS using Flume and staged data in HDFS for further analysis.\r",{"entities":[]}],["Launched and configured Amazon EC2 Cloud Servers using AMIs and configured the servers for specified applications.\r",{"entities":[]}],["Developed SQL queries to Insert, update, and delete data in a data warehouse.\r",{"entities":[]}],["Hadoop Developer NCR\r",{"entities":[[0,16,"DESIGNATION"]]}],["Atlanta, GA May 2013 - October 2015\r",{"entities":[[0,7,"LOCATION"],[9,11,"LOCATION"]]}],["Utilized Twitter API calls to get specified relevant tweeted key words from Twitter.\r",{"entities":[]}],["Installed and configured a Flume agent to ingest data from a Twitter API.\r",{"entities":[]}],["Used Sqoop to migrate data from SQL to a staging HDFS.\r",{"entities":[]}],["Migrated data using Sqoop from HDFS to Relational Database System.\r",{"entities":[]}],["Applied skill in phases of data processing (collecting, aggregating, moving from various sources) using Apache Flume and Kafka.\r",{"entities":[]}],["Implemented Capacity Schedulers on the Yarn Resource Manager to share the resources of the cluster for the jobs given by the users.\r",{"entities":[]}],["Used Spark to perform additional analytics on the DataFrame.\r",{"entities":[[5,10,"SKILLS"]]}],["Worked on Cluster coordination services through Zookeeper.\r",{"entities":[[48,57,"SKILLS"]]}],["Worked on Spark SQL to check the data.\r",{"entities":[]}],["Created modules for Spark streaming in data into Data Lake using Spark.\r",{"entities":[]}],["Automated AWS components like EC2 instances, Security groups, ELB, RDS, Lambda and IAM through AWS Cloud Formation templates.\r",{"entities":[]}],["Created multiple batch Spark jobs using Python.\r",{"entities":[]}],["Developed Sqoop jobs to populate Hive external tables using incremental loads.\r",{"entities":[]}],["Application Developer New York Library\r",{"entities":[[0,21,"DESIGNATION"],[22,30,"LOCATION"]]}],["New York, NY November 2011 - May 2013\r",{"entities":[[0,8,"LOCATION"],[10,12,"LOCATION"]]}],["Maintained and improved existing Internet/Intranet applications.\r",{"entities":[]}],["Created a workflow using technologies such as GIT/SSH to develop multi -programmer.\r",{"entities":[]}],["Wrote scripts in SQL and PL/SQL and wrote stored procedures.\r",{"entities":[]}],["Built-up and configured server cluster (CentoOS /Ubuntu).\r",{"entities":[]}],["Determined optimal business logic implementations and applied appropriate design patterns.\r",{"entities":[]}],["Developed a fully automated continuous integration system using Git, Jenkins, MySQL, and custom tools developed in Python and Bash.\r",{"entities":[]}],["Created user information solutions for backend support.\r",{"entities":[]}],["Hands on programming in Python.\r",{"entities":[]}],["Integrated development environments like Eclipse, NetBeans, and PyCharm.\r",{"entities":[]}],["Big Data, IBM\r",{"entities":[]}],["Hadoop IBM\r",{"entities":[]}],["Spark IBM\r",{"entities":[]}],["AWS Cloud Practitioner\r",{"entities":[]}],["EDUCATION\r",{"entities":[]}],["New York City College of Technology of The City University of New York - Brooklyn, NY\r",{"entities":[[0,35,"COLLEGE NAME"],[73,81,"LOCATION"],[83,85,"LOCATION"]]}],["Bachelor of Science: Computer Engineering\r",{"entities":[[0,19,"DEGREE"]]}]]}